---
title: "ihs4"
author: "Kevin Tang"
date: "6/23/2020"
output: html_document
---

```{r}

library(epiDisplay)
library(plyr)
library(magrittr)
library(foreign)
library(psych)
library(readxl)
library(tidyverse)

sdg %>% mutate(log.gdp = log(gdp))
```

Convert Non-Standard Units to KGs (data = NSU factors, HH regions)
```{r}
ihs4 <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/ihs2016/hh_mod_g_clean.csv")

unit_conv <-read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/unit_factors/ihs4factors_v5.csv")

hh_id <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/ihs2016/hh_mod_a_filt.csv")
region <- hh_id %>% select(case_id, region)

ihs4 <- merge(x=ihs4, y=unit_conv , by.x='measure_id', by.y='measure_id', fill=-9999, all.x = TRUE) %>% arrange(item_code) %>% arrange(id)

ihs4 <- merge(x=ihs4, y=region , by.x='id', by.y='case_id', fill=-9999, all.x = TRUE) %>% arrange(item_code) %>% arrange(id)

ihs4$factor_n <- ifelse(ihs4$region == 1, ihs4$factor_n, NA)
ihs4$factor_c <- ifelse(ihs4$region == 2, ihs4$factor_c, NA)
ihs4$factor_s <- ifelse(ihs4$region == 3, ihs4$factor_s, NA)
ihs4$factor <- rowSums(ihs4[,c("factor_n", "factor_c", "factor_s")], na.rm=TRUE)
ihs4 <- ihs4 %>% select(-factor_n, -factor_c, -factor_s)

ihs4$cons_kg <-ihs4$factor*ihs4$cons_quant

write.csv(ihs4, "/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/ihs2016/hh_mod_g_kg.csv")

```


>Manage outliers #1: eliminate by excess food item intake (data = HH roster)

Some individual food items may be overestimated due to stockpiling or purchases in bulk. Generally not a problem for surveys which have shorter recall periods but it doesn't hurt to check to see if there any outlandish overestimates of particular food items. In this chunk, I divided the kgs of each food items consumed by the number of people per household and the total number of days in the recall to get the total kgs consumed per capita per day. I know per capita is not the best way to divide out consumption data, but I don't think it will make up that big of a difference for this exercise and I need to push on with this part of the analysis. 

Consumption quantity generally tends to be non-normally distributed with a right skew, so to define a more approriate cut-off, we can just log-transform the distribution to normalize and then take +2SDs from the mean to identify households with unreasonable consumption estimates.
```{r}
hh_roster <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/ihs2016/hh_mod_b.csv")
percapita <- hh_roster %>% count(HHID)

ihs4 <- merge(x=ihs4, y=percapita, by.x='HHID', by.y='HHID', fill=-9999, all.x = TRUE)
ihs4$kg_pc_d <- ihs4$cons_kg/ihs4$pc/7

#Consumption (kg/capita/day) (114= spaghetti, 101= maize flour, 311=groundnuts, 408= tomatos)
ihs4 %>% filter(item_code==408) %>% ggplot(., aes(x=kg_pc_d)) + 
  geom_histogram() +
  theme_bw()

#Consumption (log(kg/capita/day))
ihs4 %>% filter(item_code==408) %>% ggplot(., aes(x=log(kg_pc_d))) + 
  geom_histogram() +
  theme_bw()


```

>Non-edible portions of food items (data = non-edible factors)

This step requires us to eliminate food weight that would have been recalled as part of the HCES questionnaire, but would not have actually been consumed due to processing or preparation. This includes parts of food like banana peels, skins of fruits and tubers that would have been peeled, bones of large fish that would have been discarded etc. There isn't any good data to estimate exactly how much weight is lost, so we have to apply an "analyst's best estimate" here. We can change these estimates if we find better data or if any new studies are conducted in the future looking specifically at this, but I honestly have no idea what funder would find this kind of work compelling enough to throw money at. We might just have to stick with the assumptions we have so far.
```{r}
waste <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/waste_factor.csv")

waste <- merge(x=fcode, y=waste , by.x='code', by.y='item_code', fill=-9999, all.x = TRUE)
waste <- waste %>% select(code, waste)

ihs4 <- merge(x=ihs4, y=waste , by.x='item_code', by.y='code', fill=-9999, all.x = TRUE) %>% arrange(HHID)

ihs4$cons_kg_w <- ihs4$cons_kg * (1-ihs4$waste)

```

>Adjustment for yield (data = yeild factors or moisture weight)

Dry weight equivalent: this has been used by Edward to calculate mineral micronutrients in previous work. Nothing wrong with the analysis he did, but I think he was able to complete that analysis with the Dry Weight Equivalent method because he didn’t conduct any estimations on any vitamins that would have been affected by the water content of the food items (i.e. energy and mineral micronutrients). For this IHS4 analysis and for the overall MAPS project, we are planning on including a number of water-soluble vitamins, such as a couple B-vitamins (B6, folate, B12) and VitC. Vitamin content of these vitamins would be a function of the water content of these foods and therefore the dry weight equivalent would not be appropriate. I see how dry weights would be advantageous for food composition scientists, especially from the BYO data features of the MAPS tool, however, advancing with this method would not account for certain vitamins of interest to us.
 
Yield factors: this method seems to be the more standard method to account for moisture in the processing and cooking of raw recalled commodities in HCES. The main problem with this method is that it assumes that the same commodity is prepared in the same way absorbing the same amount of moisture for every household. For example, in the HCES data we have the total kgs of rice consumed often recalled as a dry market purchased commodity. Yield factors can be used to calculate the kgs of cooked rice to match with the nutrient content in the FCTs, but we have no idea how our households are preparing this rice (i.e. are they steaming it or are they boiling it into a porridge) which will affect the total kgs produced from that commodity. We can assume equal preparation method across all households (i.e. all HH in Malawi steam their raw commodity rice) but this is another assumption we would need to document.
 
One thing I could do is see if we could apply different methods depending on the commodity category, as this seems to be the issue. For example, most VitC would be coming from fruits and veg, so we include moisture and match these items to fresh weight equivalents. Cereals and grains would use dry weight equivalents to account for Edwards arguments in the past email. Would be tricky with meats and legumes and a number of them contain both water and non-water soluble micronutrients, so no sure which method to apply algorithmically for those food groups…perhaps cereals and grains will use the dry weight methods while all other foods use yield factors?

```{r}
fctmatch <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/fct_matching/ihs4_fctmatch.csv")

mwifct <- read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/food_comp_tables/mwi/mwi_fct.csv")

mwifctitems <- mwifct %>% select(code, item_eng)

fcode_match <- merge(x=fcode, y=fctmatch , by.x='code', by.y='fcode', fill=-9999, all.x = TRUE) %>% select(code, item, mwi_fct_code, wa_fct_code, water_f, yield.factor, yield.assumption)

fcode_match <- merge(x=fcode_match, y=mwifctitems , by.x='mwi_fct_code', by.y='code', fill=-9999, all.x = TRUE) %>% arrange(code) %>% select(code, item, mwi_fct_code, item_eng, everything())


```

Food composition of food items (data = food composition tables)
```{r}

```

Manage outliers #2 (aggregate by total energy intake)
```{r}

```

Account for bioavailability
```{r}

```


```{r}
ihs4_health <-read.csv("/Users/kevintang/Library/Mobile Documents/com~apple~CloudDocs/Documents/LSHTM/phd epidemiology/2019 2020/data/hces/mwi/ihs2016/hh_mod_d.csv")

ihs4_health %>% count(hh_d05a)
ihs4_health %>% filter(hh_d05a==30) %>% count(case_id) %>% arrange(n)

ihs4_health %>% count(hh_d05b)
ihs4_health %>% filter(hh_d05b==30) %>% count(case_id) %>% arrange(n)
ihs4_health %>% count(hh_d05_oth)


glimpse(ihs4)
```

